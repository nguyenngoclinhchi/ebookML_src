
= CHAPTER 1: Data ingestion & inspection

== NumPy and pandas working together

Pandas depends upon and interoperates with NumPy, the Python library for
fast numeric array computations. For example, you can use the DataFrame
attribute .values to represent a DataFrame `df` as a NumPy array. You
can also pass pandas data structures to NumPy methods. In this exercise,
we have imported pandas as pd and loaded world population data every 10
years since 1960 into the DataFrame df. This dataset was derived from
the one used in the previous exercise.

Your job is to extract the values and store them in an array using the
attribute .values. You’ll then use those values as input into the NumPy
`np.log10()` method to compute the base 10 logarithm of the population
values. Finally, you will pass the entire pandas DataFrame into the same
NumPy `np.log10()` method and compare the results.


+*In[ ]:*+
[source, ipython3]
----
# Import numpy
import numpy as np
import pandas as pd

# Create array of DataFrame values: np_vals
np_vals = df.values

# Create new array of base 10 logarithm values: np_vals_log10
np_vals_log10 = np.log10(np_vals)

# Create array of new DataFrame by passing df to np.log10(): df_log10
df_log10 = np.log10(df)

# Print original and new data containers
[print(x, 'has type', type(eval(x))) for x in ['np_vals', 'np_vals_log10', 'df', 'df_log10']]
----

== Zip lists to build a DataFrame

In this exercise, you’re going to make a pandas DataFrame of the top
three countries to win gold medals since 1896 by first building a
dictionary. list_keys contains the column names `Country' and `Total'.
list_values contains the full names of each country and the number of
gold medals awarded. The values have been taken from Wikipedia.

Your job is to use these lists to construct a list of tuples, use the
list of tuples to construct a dictionary, and then use that dictionary
to construct a DataFrame. In doing so, you’ll make use of the
`list(), zip(), dict() and pd.DataFrame()` functions. Pandas has already
been imported as pd.

Note: The `zip()` function in Python 3 and above returns a special zip
object, which is essentially a generator. To convert this zip object
into a list, you’ll need to use `list()`. You can learn more about the
`zip()` function as well as generators in Python Data Science Toolbox
(Part 2).


+*In[ ]:*+
[source, ipython3]
----
# Zip the 2 lists together into one list of (key,value) tuples: zipped
zipped = list(zip(list_keys, list_values))

# Inspect the list using print()
print(zipped)

# # Build a dictionary with the zipped list: data
data = dict(zipped)

# # Build and inspect a DataFrame from the dictionary: df
df = pd.DataFrame(data)
print(df)
----

== Labeling your data

You can use the DataFrame attribute df.columns to view and assign new
string labels to columns in a pandas DataFrame.

In this exercise, we have imported pandas as pd and defined a DataFrame
df containing top Billboard hits from the 1980s (from Wikipedia). Each
row has the year, artist, song name and the number of weeks at the top.
However, this DataFrame has the column labels a, b, c, d. Your job is to
use the `df.columns` attribute to re-assign descriptive column labels


+*In[ ]:*+
[source, ipython3]
----
# Build a list of labels: list_labels
list_labels = list(['year', 'artist', 'song', 'chart weeks'])

# Assign the list of labels to the columns attribute: df.columns
df.columns = list_labels
----

== Building DataFrames with broadcasting

You can implicitly use `broadcasting', a feature of NumPy, when creating
pandas DataFrames. In this exercise, you’re going to create a DataFrame
of cities in Pennsylvania that contains the city name in one column and
the state name in the second. We have imported the names of 15 cities as
the list `cities`.

Your job is to construct a DataFrame from the list of cities and the
string `'PA'`.


+*In[ ]:*+
[source, ipython3]
----
# Make a string with the value 'PA': state
state = 'PA'

# Construct a dictionary: data
data = {'state':state, 'city':cities}

# Construct a DataFrame from dictionary data: df
df = pd.DataFrame(data)

# Print the DataFrame
print(df)
----

== Reading a flat file

In previous exercises, we have preloaded the data for you using the
pandas function `read_csv()`. Now, it’s your turn! Your job is to read
the World Bank population data you saw earlier into a DataFrame using
`read_csv()`. The file is available in the variable data_file.

The next step is to reread the same file, but simultaneously rename the
columns using the names keyword input parameter, set equal to a list of
new column labels. You will also need to set `header=0` to rename the
column labels.

Finish up by inspecting the result with `df.head() and df.info()` in the
IPython Shell (changing df to the name of your DataFrame variable).

pandas has already been imported and is available in the workspace as
pd.


+*In[ ]:*+
[source, ipython3]
----
# Read in the file: df1
df1 = pd.read_csv(data_file)

# Create a list of the new column labels: new_labels
new_labels = ['year', 'population']

# Read in the file, specifying the header and names parameters: df2
df2 = pd.read_csv(data_file, header=0, names=new_labels)

# Print both the DataFrames
print(df1)
print(df2)
----

== Delimiters, headers, and extensions

Not all data files are clean and tidy. Pandas provides methods for
reading those not-so-perfect data files that you encounter far too
often.

In this exercise, you have monthly stock data for four companies
downloaded from Yahoo Finance. The data is stored as one row for each
company and each column is the end-of-month closing price. The file name
is given to you in the variable `file_messy`.

In addition, this file has three aspects that may cause trouble for
lesser tools: multiple header lines, comment records (rows) interleaved
throughout the data rows, and space delimiters instead of commas.

Your job is to use pandas to read the data from this problematic
file_messy using non-default input options with `read_csv()` so as to
tidy up the mess at read time. Then, write the cleaned up data to a CSV
file with the variable file_clean that has been prepared for you, as you
might do in a real data workflow.

You can learn about the option input parameters needed by using `help()`
on the pandas function `pd.read_csv()`.


+*In[ ]:*+
[source, ipython3]
----
# Read the raw file as-is: df1
df1 = pd.read_csv(file_messy)

# Print the output of df1.head()
print(df1.head())

# Read in the file with the correct parameters: df2
df2 = pd.read_csv(file_messy, delimiter=' ', header=3, comment='#')

# Print the output of df2.head()
print(df2.head())

# Save the cleaned up DataFrame to a CSV file without the index
df2.to_csv(file_clean, index=False)

# Save the cleaned up DataFrame to an excel file without the index
df2.to_excel('file_clean.xlsx', index=False)
----

== Plotting series using pandas

Data visualization is often a very effective first step in gaining a
rough understanding of a data set to be analyzed. Pandas provides data
visualization by both depending upon and interoperating with the
matplotlib library. You will now explore some of the basic plotting
mechanics with pandas as well as related matplotlib options. We have
pre-loaded a pandas DataFrame df which contains the data you need. Your
job is to use the DataFrame method `df.plot()` to visualize the data,
and then explore the optional matplotlib input parameters that this
`.plot()` method accepts.

The pandas `.plot()` method makes calls to matplotlib to construct the
plots. This means that you can use the skills you’ve learned in previous
visualization courses to customize the plot. In this exercise, you’ll
add a custom title and axis labels to the figure.

Before plotting, inspect the DataFrame in the IPython Shell using
`df.head()`. Also, use `type(df)` and note that it is a single column
DataFrame.


+*In[ ]:*+
[source, ipython3]
----
import pandas as pd
import matplotlib.pyplot as plt

# Create a plot with color='red'
df.plot(color='red')

# Add a title
plt.title('Temperature in Austin')

# Specify the x-axis label
plt.xlabel('Hours since midnight August 1, 2010')

# Specify the y-axis label
plt.ylabel('Temperature (degrees F)')

# Display the plot
plt.show()
----

== Plotting DataFrames

Comparing data from several columns can be very illuminating. Pandas
makes doing so easy with multi-column DataFrames. By default, calling
`df.plot()` will cause pandas to over-plot all column data, with each
column as a single line. In this exercise, we have pre-loaded three
columns of data from a weather data set - temperature, dew point, and
pressure - but the problem is that pressure has different units of
measure. The pressure data, measured in Atmospheres, has a different
vertical scaling than that of the other two data columns, which are both
measured in degrees Fahrenheit.

Your job is to plot all columns as a multi-line plot, to see the nature
of vertical scaling problem. Then, use a list of column names passed
into the DataFrame `df[column_list]` to limit plotting to just one
column, and then just 2 columns of data. When you are finished, you will
have created 4 plots. You can cycle through them by clicking on the
`Previous Plot' and `Next Plot' buttons.

As in the previous exercise, inspect the DataFrame df in the IPython
Shell using the `.head()` and `.info()` methods.


+*In[ ]:*+
[source, ipython3]
----
# Plot all columns (default)
df.plot()
plt.show()

# Plot all columns as subplots
df.plot(subplots=True)
plt.show()

# Plot just the Dew Point data
column_list1 = ['Dew Point (deg F)']
df[column_list1].plot()
plt.show()

# Plot the Dew Point and Temperature data, but not the Pressure data
column_list2 = ['Temperature (deg F)','Dew Point (deg F)']
df[column_list2].plot()
plt.show()
----

= CHAPTER 2: Exploratory data analysis

== pandas line plots

In the previous chapter, you saw that the .plot() method will place the
Index values on the x-axis by default. In this exercise, you’ll practice
making line plots with specific columns on the x and y axes.

You will work with a dataset consisting of monthly stock prices in 2015
for AAPL, GOOG, and IBM. The stock prices were obtained from Yahoo
Finance. Your job is to plot the `Month' column on the x-axis and the
AAPL and IBM prices on the y-axis using a list of column names.

All necessary modules have been imported for you, and the DataFrame is
available in the workspace as df. Explore it using methods such as
`.head(), .info(), and .describe()` to see the column names.


+*In[ ]:*+
[source, ipython3]
----
# Create a list of y-axis column names: y_columns
y_columns = ['AAPL', 'IBM']

# Generate a line plot
df.plot(x='Month', y=y_columns)

# Add the title
plt.xlabel('Monthly stock prices')

# Add the y-axis label
plt.ylabel('Price ($US)')

# Display the plot
plt.show()
----

== pandas scatter plots

Pandas scatter plots are generated using the `kind='scatter'` keyword
argument. Scatter plots require that the x and y columns be chosen by
specifying the x and y parameters inside `.plot()`. Scatter plots also
take an s keyword argument to provide the radius of each circle to plot
in pixels.

In this exercise, you’re going to plot fuel efficiency
(miles-per-gallon) versus horse-power for 392 automobiles manufactured
from 1970 to 1982 from the UCI Machine Learning Repository.

The size of each circle is provided as a NumPy array called sizes. This
array contains the normalized `'weight'` of each automobile in the
dataset.

All necessary modules have been imported and the DataFrame is available
in the workspace as df


+*In[ ]:*+
[source, ipython3]
----
# Generate a scatter plot
df.plot(kind='scatter', x='hp', y='mpg', s=sizes)

# Add the title
plt.title('Fuel efficiency vs Horse-power')

# Add the x-axis label
plt.xlabel('Horse-power')

# Add the y-axis label
plt.ylabel('Fuel efficiency (mpg)')

# Display the plot
plt.show()
----

== pandas box plots

While pandas can plot multiple columns of data in a single figure,
making plots that share the same x and y axes, there are cases where two
columns cannot be plotted together because their units do not match. The
.plot() method can generate subplots for each column being plotted.
Here, each plot will be scaled independently.

In this exercise your job is to generate box plots for fuel efficiency
(mpg) and weight from the automobiles data set. To do this in a single
figure, you’ll specify subplots=True inside `.plot()` to generate two
separate plots.

All necessary modules have been imported and the automobiles dataset is
available in the workspace as df.


+*In[ ]:*+
[source, ipython3]
----
# Make a list of the column names to be plotted: cols
cols = ['weight', 'mpg']

# Generate the box plots
df[cols].plot(subplots=True, kind='box')

# Display the plot
plt.show()
----

== pandas hist, pdf and cdf

Pandas relies on the .hist() method to not only generate histograms, but
also plots of
`probability density functions (PDFs) and cumulative density functions (CDFs)`.

In this exercise, you will work with a dataset consisting of restaurant
bills that includes the amount customers tipped.

The original dataset is provided by the Seaborn package.

Your job is to plot a PDF and CDF for the fraction column of the tips
dataset. This column contains information about what fraction of the
total bill is comprised of the tip.

Remember, when plotting the PDF, you need to specify normed=True in your
call to `.hist()`, and when plotting the CDF, you need to specify
`cumulative=True` in addition to normed=True.

All necessary modules have been imported and the tips dataset is
available in the workspace as df. Also, some formatting code has been
written so that the plots you generate will appear on separate rows.


+*In[ ]:*+
[source, ipython3]
----
# This formats the plots such that they appear on separate rows
fig, axes = plt.subplots(nrows=2, ncols=1)

# Plot the PDF, if will appear on the first space, axes have 2 spaces
df.fraction.plot(ax=axes[0], kind='hist', normed=True, bins=30, range=(0,.3))
plt.show()

# Plot the CDF, if will appear on the second space, axes have 2 spaces 
df.fraction.plot(ax=axes[1], kind='hist', normed=True, bins=30, range=(0,.3), cumulative=True)
plt.show()
----

== Box Plot

The picture below shows the box plot structure:

image:BoxPlot.png[Box Plot]

== Bachelor’s degrees awarded to women

In this exercise, you will investigate statistics of the percentage of
Bachelor’s degrees awarded to women from 1970 to 2011. Data is recorded
every year for 17 different fields. This data set was obtained from the
Digest of Education Statistics.

Your job is to compute the minimum and maximum values of the
`'Engineering'` column and generate a line plot of the mean value of all
17 academic fields per year. To perform this step, you’ll use the
`.mean()` method with the keyword argument `axis='columns'`. This
computes the mean across all columns per row.

The DataFrame has been pre-loaded for you as df with the index set to
`'Year'`.


+*In[ ]:*+
[source, ipython3]
----
# Print the minimum value of the Engineering column
print(df['Engineering'].min())

# Print the maximum value of the Engineering column
print(df['Engineering'].max())

# Construct the mean percentage per year: mean
mean = df.mean(axis='columns')

# Plot the average percentage per year
mean.plot()

# Display the plot
plt.show()
----

== Median vs mean

In many data sets, there can be large differences in the mean and median
value due to the presence of outliers.

In this exercise, you’ll investigate the mean, median, and max fare
prices paid by passengers on the Titanic and generate a box plot of the
fare prices. This data set was obtained from Vanderbilt University.

All necessary modules have been imported and the DataFrame is available
in the workspace as df.


+*In[ ]:*+
[source, ipython3]
----
# Print summary statistics of the fare column with .describe()
print(df['fare'].describe())

# Generate a box plot of the fare column
df['fare'].plot(kind='box')

# Show the plot
plt.show()
----

== Quantiles

In this exercise, you’ll investigate the probabilities of life
expectancy in countries around the world. This dataset contains life
expectancy for persons born each year from 1800 to 2015. Since country
names change or results are not reported, not every country has values.
This dataset was obtained from Gapminder.

First, you will determine the number of countries reported in 2015.
There are a total of 260 unique countries in the entire dataset. Then,
you will compute the 5th and 95th percentiles of life expectancy over
the entire dataset. Finally, you will make a box plot of life expectancy
every 50 years from 1800 to 2000. Notice the large change in the
distributions over this period.

The dataset has been pre-loaded into a DataFrame called df


+*In[ ]:*+
[source, ipython3]
----
# Print the number of countries reported in 2015
print(df['2015'].count())

# Print the 5th and 95th percentiles
q=[0.05, 0.95]
print(df.quantile(q))

# Generate a box plot
years = ['1800','1850','1900','1950','2000']
df[years].plot(kind='box')
plt.show()
----

== Standard deviation of temperature

Let’s use the mean and standard deviation to explore differences in
temperature distributions in Pittsburgh in 2013. The data has been
obtained from Weather Underground.

In this exercise, you’re going to compare the distribution of daily
temperatures in January and March. You’ll compute the mean and standard
deviation for these two months. You will notice that while the mean
values are similar, the standard deviations are quite different, meaning
that one month had a larger fluctuation in temperature than the other.

The DataFrames have been pre-loaded for you as january, which contains
the January data, and march, which contains the March data.


+*In[ ]:*+
[source, ipython3]
----
# Print the mean of the January and March data
print(january.mean(), march.mean())

# Print the standard deviation of the January and March data
print(january.std(), march.std())
----

== Separate and summarize

Let’s use population filtering to determine how the automobiles in the
US differ from the global average and standard deviation. How does the
distribution of fuel efficiency (MPG) for the US differ from the global
average and standard deviation?

In this exercise, you’ll compute the means and standard deviations of
all columns in the full automobile dataset. Next, you’ll compute the
same quantities for just the US population and subtract the global
values from the US values.

All necessary modules have been imported and the DataFrame has been
pre-loaded as df.


+*In[ ]:*+
[source, ipython3]
----
# Compute the global mean and global standard deviation: global_mean, global_std
global_mean = df.mean()
global_std = df.std()

# Filter the US population from the origin column: us
us = (df[df['origin']=='US'])

# Compute the US mean and US standard deviation: us_mean, us_std
us_mean = us.mean()
us_std = us.std()

# Print the differences
print(us_mean - global_mean)
print(us_std - global_std)
----

== Separate and plot

Population filtering can be used alongside plotting to quickly determine
differences in distributions between the sub-populations. You’ll work
with the Titanic dataset.

There were three passenger classes on the Titanic, and passengers in
each class paid a different fare price. In this exercise, you’ll
investigate the differences in these fare prices.

Your job is to use Boolean filtering and generate box plots of the fare
prices for each of the three passenger classes. The fare prices are
contained in the `fare' column and passenger class information is
contained in the `pclass' column.

When you’re done, notice the portions of the box plots that differ and
those that are similar.

The DataFrame has been pre-loaded for you as titanic


+*In[ ]:*+
[source, ipython3]
----
# Display the box plots on 3 separate rows and 1 column
fig, axes = plt.subplots(nrows=3, ncols=1)

# Generate a box plot of the fare prices for the First passenger class
titanic.loc[titanic['pclass'] == 1].plot(ax=axes[0], y='fare', kind='box')

# Generate a box plot of the fare prices for the Second passenger class
titanic.loc[titanic['pclass'] == 2].plot(ax=axes[1], y='fare', kind='box')

# Generate a box plot of the fare prices for the Third passenger class
titanic.loc[titanic['pclass'] == 3].plot(ax=axes[2], y='fare', kind='box')

# Display the plot
plt.show()
----

= CHAPTER 3: Time Series with Pandas

== Reading and slicing times

For this exercise, we have read in the same data file using three
different approaches:

df1 = pd.read_csv(filename)

df2 = pd.read_csv(filename, parse_dates=[`Date'])

df3 = pd.read_csv(filename, index_col=`Date', parse_dates=True)

Use the .head() and .info() methods in the IPython Shell to inspect the
DataFrames. Then, try to index each DataFrame with a datetime string.
Which of the resulting DataFrames allows you to easily index and slice
data by dates using, for example, `df1.loc['2010-Aug-01']`?

–> `ANS: df3`

== Creating and using a DatetimeIndex

The pandas Index is a powerful way to handle time series data, so it is
valuable to know how to build one yourself. Pandas provides the
`pd.to_datetime()` function for just this task. For example, if passed
the list of strings `['2015-01-01 091234','2015-01-01 091234']` and a
format specification variable, such as `format='%Y-%m-%d %H%M%S`, pandas
will parse the string into the proper datetime elements and build the
datetime objects.

In this exercise, a list of temperature data and a list of date strings
has been pre-loaded for you as `temperature_list` and date_list
respectively. Your job is to use the `.to_datetime()` method to build a
DatetimeIndex out of the list of date strings, and to then use it along
with the list of temperature data to build a pandas `Series`.


+*In[ ]:*+
[source, ipython3]
----
# Prepare a format string: time_format
time_format = '%Y-%m-%d %H:%M'

# Convert date_list into a datetime object: my_datetimes
my_datetimes = pd.to_datetime(date_list, format=time_format)  

# Construct a pandas Series using temperature_list and my_datetimes: time_series
time_series = pd.Series(temperature_list, index=my_datetimes)
----

== Partial string indexing and slicing

Pandas time series support ``partial string'' indexing. What this means
is that even when passed only a portion of the datetime, such as the
date but not the time, pandas is remarkably good at doing what one would
expect. Pandas datetime indexing also supports a wide variety of
commonly used datetime string formats, even when mixed.

In this exercise, a time series that contains hourly weather data has
been pre-loaded for you. This data was read using the parse_dates=True
option in read_csv() with index_col=``Dates'' so that the Index is
indeed a DatetimeIndex.

All data from the `Temperature' column has been extracted into the
variable ts0. Your job is to use a variety of natural date strings to
extract one or more values from ts0.

After you are done, you will have three new variables - ts1, ts2, and
ts3. You can slice these further to extract only the first and last
entries of each. Try doing this after your submission for more practice.


+*In[ ]:*+
[source, ipython3]
----
# Extract the hour from 9pm to 10pm on '2010-10-11': ts1
ts1 = ts0.loc['2010-10-11 21:00:00':'2010-10-11 22:00:00']

# Extract '2010-07-04' from ts0: ts2
ts2 = ts0.loc['2010-07-04']

# Extract data from '2010-12-15' to '2010-12-31': ts3
ts3 = ts0.loc['2010-12-15':'2010-12-31']

----

== Resampling and frequency

Pandas provides methods for resampling time series data. When
downsampling or upsampling, the syntax is similar, but the methods
called are different. Both use the concept of
`'method chaining' - df.method1().method2().method3()` - to direct the
output from one method call to the input of the next, and so on, as a
sequence of operations, one feeding into the next.

For example, if you have hourly data, and just need daily data, pandas
will not guess how to throw out the 23 of 24 points. You must specify
this in the method. One approach, for instance, could be to take the
mean, as in `df.resample('D').mean()`.

In this exercise, a data set containing hourly temperature data has been
pre-loaded for you. Your job is to resample the data using a variety of
aggregation methods to answer a few questions.


+*In[ ]:*+
[source, ipython3]
----
# Downsample to 6 hour data and aggregate by mean: df1
df1 = df['Temperature'].resample('6h').mean()
print(df)
print(df1)
# Downsample to daily data and count the number of data points: df2
df2 = df['Temperature'].resample('D').count()
print(df2)
----

== Separating and resampling

With pandas, you can resample in different ways on different subsets of
your data. For example, resampling different months of data with
different aggregations. In this exercise, the data set containing hourly
temperature data from the last exercise has been pre-loaded.

Your job is to resample the data using a variety of aggregation methods.
The DataFrame is available in the workspace as df. You will be working
with the `'Temperature'` column.


+*In[ ]:*+
[source, ipython3]
----
# Extract temperature data for August: august
august = (df['Temperature']).loc['2010-08']

# Downsample to obtain only the daily highest temperatures in August: august_highs
august_highs = august.resample('D').max()

# Extract temperature data for February: february
february = (df['Temperature']).loc['2010-02']

# Downsample to obtain the daily lowest temperatures in February: february_lows
february_lows = february.resample('D').min()
----

== Rolling mean and frequency

In this exercise, some hourly weather data is pre-loaded for you. You
will continue to practice resampling, this time using rolling means.

Rolling means (or moving averages) are generally used to smooth out
short-term fluctuations in time series data and highlight long-term
trends. You can read more about them here.

To use the `.rolling()` method, you must always use method chaining,
first calling `.rolling()` and then chaining an aggregation method after
it. For example, with a Series hourly_data,
`hourly_data.rolling(window=24).mean()` would compute new values for
each hourly point, based on a 24-hour window stretching out behind each
point. The frequency of the output data is the same: it is still hourly.
Such an operation is useful for smoothing time series data.

Your job is to resample the data using the combination of `.rolling()`
and `.mean()`. You will work with the same DataFrame df from the
previous exercise.


+*In[ ]:*+
[source, ipython3]
----
# Extract data from 2010-Aug-01 to 2010-Aug-15: unsmoothed
unsmoothed = df['Temperature']['2010-Aug-01':'2010-Aug-15']

# Apply a rolling mean with a 24 hour window: smoothed
smoothed = unsmoothed.rolling(window=24).mean()

# Create a new DataFrame with columns smoothed and unsmoothed: august
august = pd.DataFrame({'smoothed':smoothed, 'unsmoothed':unsmoothed})

# Plot both smoothed and unsmoothed data using august.plot().
august.plot()
plt.show()
----

== Resample and roll with it

As of pandas version 0.18.0, the interface for applying rolling
transformations to time series has become more consistent and flexible,
and feels somewhat like a groupby (If you do not know what a groupby is,
don’t worry, you will learn about it in the next course!).

You can now flexibly chain together resampling and rolling operations.
In this exercise, the same weather data from the previous exercises has
been pre-loaded for you. Your job is to extract one month of data,
resample to find the daily high temperatures, and then use a rolling and
aggregation operation to smooth the data.


+*In[ ]:*+
[source, ipython3]
----
import matplotlib.pyplot as plt
# Extract the August 2010 data: august
august = df['Temperature']['2010-Aug']

# Resample to daily data, aggregating by max: daily_highs
daily_highs = august.resample('D').max()

# Use a rolling 7-day window with method chaining to smooth the daily high temperatures in August
daily_highs_smoothed = daily_highs.rolling(window=7).mean()
print(daily_highs_smoothed)
----

== Method chaining and filtering

We’ve seen that pandas supports method chaining. This technique can be
very powerful when cleaning and filtering data.

In this exercise, a DataFrame containing flight departure data for a
single airline and a single airport for the month of July 2015 has been
pre-loaded. Your job is to use `.str()` filtering and method chaining to
generate summary statistics on flight delays each day to Dallas.


+*In[ ]:*+
[source, ipython3]
----
# Strip extra whitespace from the column names: df.columns
df.columns = df.columns.str.strip()

# Extract data for which the destination airport is Dallas: dallas
dallas = df['Destination Airport'].str.contains("DAL")

# Compute the total number of Dallas departures each day: daily_departures
daily_departures = dallas.resample('D').sum()
print(daily_departures)

# Generate the summary statistics for daily Dallas departures: stats
stats = daily_departures.describe()
----

== Missing values and interpolation

One common application of interpolation in data analysis is to fill in
missing data.

In this exercise, noisy measured data that has some dropped or otherwise
missing values has been loaded. The goal is to compare two time series,
and then look at summary statistics of the differences. The problem is
that one of the data sets is missing data at some of the times. The
pre-loaded data ts1 has value for all times, yet the data set ts2 does
not: it is missing data for the weekends.

Your job is to first interpolate to fill in the data for all days. Then,
compute the differences between the two data sets, now that they both
have full support for all times. Finally, generate the summary
statistics that describe the distribution of differences.


+*In[ ]:*+
[source, ipython3]
----
# Reset the index of ts2 to ts1, and then use linear interpolation to fill in the NaNs: ts2_interp
ts2_interp = pd.Series(ts2, index=ts1).interpolate(how='linear')

# Compute the absolute difference of ts1 and ts2_interp: differences 
differences = np.abs(ts1 - ts2_interp)

# Generate and print summary statistics of the differences
print(differences)
----

== Missing values and interpolation

One common application of interpolation in data analysis is to fill in
missing data.

In this exercise, noisy measured data that has some dropped or otherwise
missing values has been loaded. The goal is to compare two time series,
and then look at summary statistics of the differences. The problem is
that one of the data sets is missing data at some of the times. The
pre-loaded data ts1 has value for all times, yet the data set ts2 does
not: it is missing data for the weekends.

Your job is to first interpolate to fill in the data for all days. Then,
compute the differences between the two data sets, now that they both
have full support for all times. Finally, generate the summary
statistics that describe the distribution of differences.


+*In[ ]:*+
[source, ipython3]
----
# Reset the index of ts2 to ts1, and then use linear interpolation to fill in the NaNs: ts2_interp
ts2_interp = ts2.reindex(ts1.index).interpolate(how='linear')

# Compute the absolute difference of ts1 and ts2_interp: differences 
differences = np.abs(ts1 - ts2_interp)

# Generate and print summary statistics of the differences
print(differences.describe())
----

== Time zones and conversion

Time zone handling with pandas typically assumes that you are handling
the Index of the Series. In this exercise, you will learn how to handle
timezones that are associated with datetimes in the column data, and not
just the Index.

You will work with the flight departure dataset again, and this time you
will select Los Angeles (`LAX') as the destination airport.

Here we will use a mask to ensure that we only compute on data we
actually want. To learn more about Boolean masks, click here!


+*In[ ]:*+
[source, ipython3]
----
# Build a Boolean mask to filter out all the 'LAX' departure flights: mask
mask = df['Destination Airport'] == 'LAX'

# Use the mask to subset the data: la
la = df[mask]

# Combine two columns of data to create a datetime series: times_tz_none 
times_tz_none = pd.to_datetime(la['Date (MM/DD/YYYY)'] + ' ' + la['Wheels-off Time'])

# Localize the time to US/Central: times_tz_central
times_tz_central = times_tz_none.dt.tz_localize(tz='US/Central')

# Convert the datetimes from US/Central to US/Pacific
times_tz_pacific = times_tz_central.dt.tz_convert(tz='US/Pacific')
----

== Plotting time series, datetime indexing

Pandas handles datetimes not only in your data, but also in your
plotting.

In this exercise, some time series data has been pre-loaded. However, we
have not parsed the date-like columns nor set the index, as we have done
for you in the past!

The plot displayed is how pandas renders data with the default
integer/positional index. Your job is to convert the `Date' column from
a collection of strings into a collection of datetime objects. Then, you
will use this converted `Date' column as your new index, and re-plot the
data, noting the improved datetime awareness. After you are done, you
can cycle between the two plots you generated by clicking on the
`Previous Plot' and `Next Plot' buttons.

Before proceeding, look at the plot shown and observe how pandas handles
data with the default integer index. Then, inspect the DataFrame df
using the `.head()` method in the IPython Shell to get a feel for its
structure.


+*In[ ]:*+
[source, ipython3]
----
# Plot the raw data before setting the datetime index
df1 = df
df.head()
df.plot()
plt.show()

time_format = '%Y-%m-%d %H:%M'

# Convert the 'Date' column into a collection of datetime objects: df.Date
pd.to_datetime(df.Date) 

# Set the index to be the converted 'Date' column
df.set_index('Date', inplace=True)
# print(df['Date'])
# Re-plot the DataFrame to see that the axis is now datetime aware!
df.plot()
plt.show()
----

== Plotting date ranges, partial indexing

Now that you have set the DatetimeIndex in your DataFrame, you have a
much more powerful and flexible set of tools to use when plotting your
time series data. Of these, one of the most convenient is partial string
indexing and slicing. In this exercise, we’ve pre-loaded a full year of
Austin 2010 weather data, with the index set to be the datetime parsed
`Date' column as shown in the previous exercise.

Your job is to use partial string indexing of the dates, in a variety of
datetime string formats, to plot all the summer data and just one week
of data together. After you are done, you can cycle between the two
plots by clicking on the `Previous Plot' and `Next Plot' buttons.

First, remind yourself how to extract one month of temperature data
using `May 2010' as a key into `df.Temperature[]`, and call head() to
inspect the result: `df.Temperature['May 2010'].head()`.


+*In[ ]:*+
[source, ipython3]
----
# Plot the summer data
df.Temperature['2010-Jun':'2010-Aug'].plot()
plt.show()
plt.clf()

# Plot the one week data
df.Temperature['2010-06-10':'2010-06-17'].plot()
plt.show()
plt.clf()
----

= Chapter 4: CASE STUDY

== Reading in a data file

Now that you have identified the method to use to read the data, let’s
try to read one file. The problem with real data such as this is that
the files are almost never formatted in a convenient way. In this
exercise, there are several problems to overcome in reading the file.
First, there is no header, and thus the columns don’t have labels. There
is also no obvious index column, since none of the data columns contain
a full date or time.

Your job is to read the file into a DataFrame using the default
arguments. After inspecting it, you will re-read the file specifying
that there are no headers supplied.

The CSV file has been provided for you as the variable `data_file`


+*In[ ]:*+
[source, ipython3]
----
# Import pandas
import pandas as pd

# Read in the data file: df
df = pd.read_csv(data_file)

# Print the output of df.head()
print(df.head())

# Read in the data file with header=None: df_headers
df_headers = pd.read_csv(data_file, header=None)

# Print the output of df_headers.head()
print(df_headers.head())
----


+*In[ ]:*+
[source, ipython3]
----

----
